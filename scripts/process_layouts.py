from collections import Counter
import json
from pathlib import Path
from itertools import chain

import networkx as nx
import numpy as np
import pandas as pd

from colorcet import glasbey_dark


# directory with GraphML files generated by Gephi
INPUT_DIR = Path('../graphml/')

OUTPUT_DIR = Path('../')


def process_graphml_to_dataframes(input_file):

    # Load the laid-out GraphML file saved from Gephi into a NetworkX graph
    G = nx.read_graphml(path = input_file)

    # Extract node and edge information from NetworkX graph into raw DataFrames
    _nodes_df = pd.DataFrame.from_dict(
        dict(G.nodes(data=True)), orient='index'
      ).reset_index(drop = True)
    _edges_df = nx.to_pandas_edgelist(G = G)

    # Set column order of nodes DataFrame and ignore irrelevant attributes
    nodes_df = _nodes_df[['x', 'y', 'size', 'views', 'views_root_3', 'label', 'category']]
    # Create dict that maps node name to a numeric index
    node_names = sorted(nodes_df['label'])
    name_to_index = dict( zip(node_names, np.arange(len(node_names))))
    # Create a column in the nodes DataFrame for the numeric index (this is necessary for HoloViews visualization)
    nodes_df.insert(loc = 2, column = 'index', value = nodes_df['label'].map(name_to_index))

    # truncate values or round
    nodes_df["views"] = nodes_df["views"].astype(int)
    nodes_df = nodes_df.round(1)

    # Remove the unnecessary "id" column from the edges DataFrame
    edges_df = _edges_df.drop('id', axis = 'columns')

    return nodes_df, edges_df

def map_name_to_index(all_dataframes):
    all_edge_nodes = []
    for e in all_dataframes.values():
        for edge in e[1][["source", "target"]].values.tolist():
            all_edge_nodes.extend(edge)
    c = Counter(all_edge_nodes)
    name_to_index = dict(zip(map(lambda e : e[0], c.most_common()), range(len(c))))
    return name_to_index

def reindex_dataframes(nodes_df, edges_df, name_to_index):
    nodes_df["index"] = nodes_df["label"].map(name_to_index)
    edges_df['source'] = edges_df['source'].map(name_to_index)
    edges_df['target'] = edges_df['target'].map(name_to_index)
    return nodes_df, edges_df

if __name__ == "__main__":

    all_dataframes = {}
    for agg in ["bipartite", "guests"]:
        for threshold in range(4):
            input_filename = f"{agg}_podcasts_layout_threshold={threshold}.graphml"
            all_dataframes[f"{agg}_{threshold}"] = process_graphml_to_dataframes(
                input_file = INPUT_DIR / input_filename)

    name_to_index = map_name_to_index(all_dataframes = all_dataframes)

    all_dataframes = {k : reindex_dataframes(nodes_df, edges_df, name_to_index) for k, (nodes_df, edges_df) in all_dataframes.items()}
    all_nodes = all_dataframes["bipartite_0"][0].sort_values("index")[["label", "views", "category"]].values.tolist()

    full_network_data = {}
    full_network_data["allNodes"] = all_nodes
    full_network_data["layouts"] = {}
    full_network_data["edges"] = {}
    for threshold, (nodes_df, edges_df) in all_dataframes.items():
        print(threshold)
        layout = [[int(index), x, y] for _, (index, x, y) in nodes_df[["index", "x", "y"]].iterrows()]
        edges = edges_df[["source", "target"]].values.tolist()
        full_network_data["layouts"][threshold] = layout
        full_network_data["edges"][threshold] = edges

    full_network_data["radiusScaling"] = {
        0: 0.02,
        1: 0.03,
        2: 0.04,
        3: 0.055
    }

    full_network_data["textScaling"] = {
        0: 0.02,
        1: 0.03,
        2: 0.03,
        3: 0.03
    }

    with open(OUTPUT_DIR / "full_network_data_1.json", "w") as f:
        json.dump(obj = full_network_data, fp = f, separators=(',', ':'))
